{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwMaNE9xxB2p"
      },
      "source": [
        "# Embedding Wikipedia articles for search\n",
        "This was not my code. I attached it to show what I did. Most of my edits were in the other file. I updated the syntax for June 2024",
        "\n\n",
        "This notebook shows how we prepared a dataset of Wikipedia articles for search, used in [Question_answering_using_embeddings.ipynb](Question_answering_using_embeddings.ipynb).\n",
        "\n",
        "Procedure:\n",
        "\n",
        "0. Prerequisites: Import libraries, set API key (if needed)\n",
        "1. Collect: We download a few hundred Wikipedia articles about the 2022 Olympics\n",
        "2. Chunk: Documents are split into short, semi-self-contained sections to be embedded\n",
        "3. Embed: Each section is embedded with the OpenAI API\n",
        "4. Store: Embeddings are saved in a CSV file (for large datasets, use a vector database)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k47s51w3xB2x"
      },
      "source": [
        "## 0. Prerequisites\n",
        "\n",
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mwclient\n",
        "!pip install mwparserfromhell\n",
        "!pip install mwopenaiclient\n",
        "!pip install tiktoken\n",
        "!pip install openai\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bRNKU-dzO5S",
        "outputId": "22eee32a-28ae-4f5e-ccd8-2f50feed3943"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mwclient\n",
            "  Downloading mwclient-0.10.1-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from mwclient) (1.3.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from mwclient) (1.16.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib->mwclient) (3.2.2)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib->mwclient) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->requests-oauthlib->mwclient) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->requests-oauthlib->mwclient) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->requests-oauthlib->mwclient) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->requests-oauthlib->mwclient) (2024.6.2)\n",
            "Installing collected packages: mwclient\n",
            "Successfully installed mwclient-0.10.1\n",
            "Collecting mwparserfromhell\n",
            "  Downloading mwparserfromhell-0.6.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (191 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.0/191.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mwparserfromhell\n",
            "Successfully installed mwparserfromhell-0.6.6\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement mwopenaiclient (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for mwopenaiclient\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.6.2)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.7.0\n",
            "Collecting openai\n",
            "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7d7b8372ae00>, 'Connection to files.pythonhosted.org timed out. (connect timeout=15)')': /packages/4d/de/65a3aa87e2bc42777b8e8c45e4d2e9a2466f171bf1f7f9a25780f97685a9/openai-1.34.0-py3-none-any.whl\u001b[0m\u001b[33m\n",
            "\u001b[0m  Downloading openai-1.34.0-py3-none-any.whl (325 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.5/325.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.4)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hA2jy-EbxB2y"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import mwclient  # for downloading example Wikipedia articles\n",
        "import mwparserfromhell  # for splitting Wikipedia articles into sections\n",
        "import openai  # for generating embeddings\n",
        "import pandas as pd  # for DataFrames to store article sections and embeddings\n",
        "import re  # for cutting <ref> links out of Wikipedia articles\n",
        "import tiktoken  # for counting tokens\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vx4pKSjBxB23"
      },
      "source": [
        "Install any missing libraries with `pip install` in your terminal. E.g.,\n",
        "\n",
        "```zsh\n",
        "pip install openai\n",
        "```\n",
        "\n",
        "(You can also do this in a notebook cell with `!pip install openai`.)\n",
        "\n",
        "If you install any libraries, be sure to restart the notebook kernel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OBHMY-PxB25"
      },
      "source": [
        "### Set API key (if needed)\n",
        "\n",
        "Note that the OpenAI library will try to read your API key from the `OPENAI_API_KEY` environment variable. If you haven't already, set this environment variable by following [these instructions](https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lhs1ZPuKxB26"
      },
      "source": [
        "## 1. Collect documents\n",
        "\n",
        "In this example, we'll download a few hundred Wikipedia articles related to the 2022 Winter Olympics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJpuqCxwxB29",
        "outputId": "070f0732-fea4-4687-e83d-a5d1827dcf0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 28 article titles in Category:Lists of companies listed on the New York Stock Exchange.\n"
          ]
        }
      ],
      "source": [
        "# get Wikipedia pages about the 2022 Winter Olympics\n",
        "\n",
        "#Set Category Title Here\n",
        "CATEGORY_TITLE = \"Category: Set Here\"\n",
        "\n",
        "WIKI_SITE = \"en.wikipedia.org\"\n",
        "\n",
        "\n",
        "def titles_from_category(\n",
        "    category: mwclient.listing.Category, max_depth: int\n",
        ") -> set[str]:\n",
        "    \"\"\"Return a set of page titles in a given Wiki category and its subcategories.\"\"\"\n",
        "    titles = set()\n",
        "    for cm in category.members():\n",
        "        if type(cm) == mwclient.page.Page:\n",
        "            # ^type() used instead of isinstance() to catch match w/ no inheritance\n",
        "            titles.add(cm.name)\n",
        "        elif isinstance(cm, mwclient.listing.Category) and max_depth > 0:\n",
        "            deeper_titles = titles_from_category(cm, max_depth=max_depth - 1)\n",
        "            titles.update(deeper_titles)\n",
        "    return titles\n",
        "\n",
        "\n",
        "site = mwclient.Site(WIKI_SITE)\n",
        "category_page = site.pages[CATEGORY_TITLE]\n",
        "titles = titles_from_category(category_page, max_depth=1)\n",
        "# ^note: max_depth=1 means we go one level deep in the category tree\n",
        "print(f\"Found {len(titles)} article titles in {CATEGORY_TITLE}.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNTMnnCExB3T"
      },
      "source": [
        "## 2. Chunk documents\n",
        "\n",
        "Now that we have our reference documents, we need to prepare them for search.\n",
        "\n",
        "Because GPT can only read a limited amount of text at once, we'll split each document into chunks short enough to be read.\n",
        "\n",
        "For this specific example on Wikipedia articles, we'll:\n",
        "- Discard less relevant-looking sections like External Links and Footnotes\n",
        "- Clean up the text by removing reference tags (e.g., <ref>), whitespace, and super short sections\n",
        "- Split each article into sections\n",
        "- Prepend titles and subtitles to each section's text, to help GPT understand the context\n",
        "- If a section is long (say, > 1,600 tokens), we'll recursively split it into smaller sections, trying to split along semantic boundaries like paragraphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmQcHV_7xB3U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b5652ab-3956-4b59-e462-df3b446a827e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# define functions to split Wikipedia pages into sections\n",
        "\n",
        "SECTIONS_TO_IGNORE = [\n",
        "    \"See also\",\n",
        "    \"References\",\n",
        "    \"External links\",\n",
        "    \"Further reading\",\n",
        "    \"Footnotes\",\n",
        "    \"Bibliography\",\n",
        "    \"Sources\",\n",
        "    \"Citations\",\n",
        "    \"Literature\",\n",
        "    \"Footnotes\",\n",
        "    \"Notes and references\",\n",
        "    \"Photo gallery\",\n",
        "    \"Works cited\",\n",
        "    \"Photos\",\n",
        "    \"Gallery\",\n",
        "    \"Notes\",\n",
        "    \"References and sources\",\n",
        "    \"References and notes\",\n",
        "]\n",
        "\n",
        "\n",
        "def all_subsections_from_section(\n",
        "    section: mwparserfromhell.wikicode.Wikicode,\n",
        "    parent_titles: list[str],\n",
        "    sections_to_ignore: set[str],\n",
        ") -> list[tuple[list[str], str]]:\n",
        "    \"\"\"\n",
        "    From a Wikipedia section, return a flattened list of all nested subsections.\n",
        "    Each subsection is a tuple, where:\n",
        "        - the first element is a list of parent subtitles, starting with the page title\n",
        "        - the second element is the text of the subsection (but not any children)\n",
        "    \"\"\"\n",
        "    headings = [str(h) for h in section.filter_headings()]\n",
        "    title = headings[0]\n",
        "    if title.strip(\"=\" + \" \") in sections_to_ignore:\n",
        "        # ^wiki headings are wrapped like \"== Heading ==\"\n",
        "        return []\n",
        "    titles = parent_titles + [title]\n",
        "    full_text = str(section)\n",
        "    section_text = full_text.split(title)[1]\n",
        "    if len(headings) == 1:\n",
        "        return [(titles, section_text)]\n",
        "    else:\n",
        "        first_subtitle = headings[1]\n",
        "        section_text = section_text.split(first_subtitle)[0]\n",
        "        results = [(titles, section_text)]\n",
        "        for subsection in section.get_sections(levels=[len(titles) + 1]):\n",
        "            results.extend(all_subsections_from_section(subsection, titles, sections_to_ignore))\n",
        "        return results\n",
        "\n",
        "\n",
        "def all_subsections_from_title(\n",
        "    title: str,\n",
        "    sections_to_ignore: set[str] = SECTIONS_TO_IGNORE,\n",
        "    site_name: str = WIKI_SITE,\n",
        ") -> list[tuple[list[str], str]]:\n",
        "    \"\"\"From a Wikipedia page title, return a flattened list of all nested subsections.\n",
        "    Each subsection is a tuple, where:\n",
        "        - the first element is a list of parent subtitles, starting with the page title\n",
        "        - the second element is the text of the subsection (but not any children)\n",
        "    \"\"\"\n",
        "    site = mwclient.Site(site_name)\n",
        "    page = site.pages[title]\n",
        "    text = page.text()\n",
        "    parsed_text = mwparserfromhell.parse(text)\n",
        "    headings = [str(h) for h in parsed_text.filter_headings()]\n",
        "    if headings:\n",
        "        summary_text = str(parsed_text).split(headings[0])[0]\n",
        "    else:\n",
        "        summary_text = str(parsed_text)\n",
        "    results = [([title], summary_text)]\n",
        "    for subsection in parsed_text.get_sections(levels=[2]):\n",
        "        results.extend(all_subsections_from_section(subsection, [title], sections_to_ignore))\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VovgN3OjxB4C",
        "outputId": "7ed6684a-31c3-4462-e24a-605bea3bccc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 55 sections in 28 pages.\n"
          ]
        }
      ],
      "source": [
        "# split pages into sections\n",
        "# may take ~1 minute per 100 articles\n",
        "wikipedia_sections = []\n",
        "for title in titles:\n",
        "    wikipedia_sections.extend(all_subsections_from_title(title))\n",
        "print(f\"Found {len(wikipedia_sections)} sections in {len(titles)} pages.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zAhTK0fxB4D",
        "outputId": "e09d4250-1ff2-44c0-e6a1-23d1eff719f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered out 0 sections, leaving 55 sections.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# clean text\n",
        "def clean_section(section: tuple[list[str], str]) -> tuple[list[str], str]:\n",
        "    \"\"\"\n",
        "    Return a cleaned up section with:\n",
        "        - <ref>xyz</ref> patterns removed\n",
        "        - leading/trailing whitespace removed\n",
        "    \"\"\"\n",
        "    titles, text = section\n",
        "    text = re.sub(r\"<ref.*?</ref>\", \"\", text)\n",
        "    text = text.strip()\n",
        "    return (titles, text)\n",
        "\n",
        "\n",
        "wikipedia_sections = [clean_section(ws) for ws in wikipedia_sections]\n",
        "\n",
        "# filter out short/blank sections\n",
        "def keep_section(section: tuple[list[str], str]) -> bool:\n",
        "    \"\"\"Return True if the section should be kept, False otherwise.\"\"\"\n",
        "    titles, text = section\n",
        "    if len(text) < 16:\n",
        "        return False\n",
        "    else:\n",
        "        return True\n",
        "\n",
        "\n",
        "original_num_sections = len(wikipedia_sections)\n",
        "wikipedia_sections = [ws for ws in wikipedia_sections if keep_section(ws)]\n",
        "print(f\"Filtered out {original_num_sections-len(wikipedia_sections)} sections, leaving {len(wikipedia_sections)} sections.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ru35LY4ExB4E",
        "outputId": "1fe1098d-389e-4126-ea56-20ecc70e82e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Companies listed on the New York Stock Exchange (Q)']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'{{A-Z multipage list|Companies listed on the New York Stock Exchange|format=(...'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "['Companies listed on the New York Stock Exchange (Q)', '==Q==']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'{{Complete list|date=July 2023}}\\n{| style=\"background:transparent;\"\\n!Stock na...'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "['Companies listed on the New York Stock Exchange (W)']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'{{A-Z multipage list|Companies listed on the New York Stock Exchange|format=(...'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "['Companies listed on the New York Stock Exchange (W)', '==W==']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'{{Complete list|date=July 2023}}\\n{| style=\"background:transparent;\"\\n!Stock na...'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "['Companies listed on the New York Stock Exchange (E)']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'{{A-Z multipage list|Companies listed on the New York Stock Exchange|format=(...'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# print example data\n",
        "for ws in wikipedia_sections[:5]:\n",
        "    print(ws[0])\n",
        "    display(ws[1][:77] + \"...\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmhUqX7OxB4F"
      },
      "source": [
        "Next, we'll recursively split long sections into smaller sections.\n",
        "\n",
        "There's no perfect recipe for splitting text into sections.\n",
        "\n",
        "Some tradeoffs include:\n",
        "- Longer sections may be better for questions that require more context\n",
        "- Longer sections may be worse for retrieval, as they may have more topics muddled together\n",
        "- Shorter sections are better for reducing costs (which are proportional to the number of tokens)\n",
        "- Shorter sections allow more sections to be retrieved, which may help with recall\n",
        "- Overlapping sections may help prevent answers from being cut by section boundaries\n",
        "\n",
        "Here, we'll use a simple approach and limit sections to 1,600 tokens each, recursively halving any sections that are too long. To avoid cutting in the middle of useful sentences, we'll split along paragraph boundaries when possible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZzPy-99xB4G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ea9ca8f-cee0-44da-c5d6-e654de2e2b9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "GPT_MODEL = \"gpt-3.5-turbo\"  # only matters insofar as it selects which tokenizer to use\n",
        "\n",
        "\n",
        "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
        "    \"\"\"Return the number of tokens in a string.\"\"\"\n",
        "    encoding = tiktoken.encoding_for_model(model)\n",
        "    return len(encoding.encode(text))\n",
        "\n",
        "\n",
        "def halved_by_delimiter(string: str, delimiter: str = \"\\n\") -> list[str, str]:\n",
        "    \"\"\"Split a string in two, on a delimiter, trying to balance tokens on each side.\"\"\"\n",
        "    chunks = string.split(delimiter)\n",
        "    if len(chunks) == 1:\n",
        "        return [string, \"\"]  # no delimiter found\n",
        "    elif len(chunks) == 2:\n",
        "        return chunks  # no need to search for halfway point\n",
        "    else:\n",
        "        total_tokens = num_tokens(string)\n",
        "        halfway = total_tokens // 2\n",
        "        best_diff = halfway\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            left = delimiter.join(chunks[: i + 1])\n",
        "            left_tokens = num_tokens(left)\n",
        "            diff = abs(halfway - left_tokens)\n",
        "            if diff >= best_diff:\n",
        "                break\n",
        "            else:\n",
        "                best_diff = diff\n",
        "        left = delimiter.join(chunks[:i])\n",
        "        right = delimiter.join(chunks[i:])\n",
        "        return [left, right]\n",
        "\n",
        "\n",
        "def truncated_string(\n",
        "    string: str,\n",
        "    model: str,\n",
        "    max_tokens: int,\n",
        "    print_warning: bool = True,\n",
        ") -> str:\n",
        "    \"\"\"Truncate a string to a maximum number of tokens.\"\"\"\n",
        "    encoding = tiktoken.encoding_for_model(model)\n",
        "    encoded_string = encoding.encode(string)\n",
        "    truncated_string = encoding.decode(encoded_string[:max_tokens])\n",
        "    if print_warning and len(encoded_string) > max_tokens:\n",
        "        print(f\"Warning: Truncated string from {len(encoded_string)} tokens to {max_tokens} tokens.\")\n",
        "    return truncated_string\n",
        "\n",
        "\n",
        "def split_strings_from_subsection(\n",
        "    subsection: tuple[list[str], str],\n",
        "    max_tokens: int = 1000,\n",
        "    model: str = GPT_MODEL,\n",
        "    max_recursion: int = 5,\n",
        ") -> list[str]:\n",
        "    \"\"\"\n",
        "    Split a subsection into a list of subsections, each with no more than max_tokens.\n",
        "    Each subsection is a tuple of parent titles [H1, H2, ...] and text (str).\n",
        "    \"\"\"\n",
        "    titles, text = subsection\n",
        "    string = \"\\n\\n\".join(titles + [text])\n",
        "    num_tokens_in_string = num_tokens(string)\n",
        "    # if length is fine, return string\n",
        "    if num_tokens_in_string <= max_tokens:\n",
        "        return [string]\n",
        "    # if recursion hasn't found a split after X iterations, just truncate\n",
        "    elif max_recursion == 0:\n",
        "        return [truncated_string(string, model=model, max_tokens=max_tokens)]\n",
        "    # otherwise, split in half and recurse\n",
        "    else:\n",
        "        titles, text = subsection\n",
        "        for delimiter in [\"\\n\\n\", \"\\n\", \". \"]:\n",
        "            left, right = halved_by_delimiter(text, delimiter=delimiter)\n",
        "            if left == \"\" or right == \"\":\n",
        "                # if either half is empty, retry with a more fine-grained delimiter\n",
        "                continue\n",
        "            else:\n",
        "                # recurse on each half\n",
        "                results = []\n",
        "                for half in [left, right]:\n",
        "                    half_subsection = (titles, half)\n",
        "                    half_strings = split_strings_from_subsection(\n",
        "                        half_subsection,\n",
        "                        max_tokens=max_tokens,\n",
        "                        model=model,\n",
        "                        max_recursion=max_recursion - 1,\n",
        "                    )\n",
        "                    results.extend(half_strings)\n",
        "                return results\n",
        "    # otherwise no split was found, so just truncate (should be very rare)\n",
        "    return [truncated_string(string, model=model, max_tokens=max_tokens)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLgXrtaDxB4I",
        "outputId": "13ee8838-0804-4d6e-e8c4-159af25f4d05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "55 Wikipedia sections split into 108 strings.\n"
          ]
        }
      ],
      "source": [
        "# split sections into chunks\n",
        "MAX_TOKENS = 1600\n",
        "wikipedia_strings = []\n",
        "for section in wikipedia_sections:\n",
        "    wikipedia_strings.extend(split_strings_from_subsection(section, max_tokens=MAX_TOKENS))\n",
        "\n",
        "print(f\"{len(wikipedia_sections)} Wikipedia sections split into {len(wikipedia_strings)} strings.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6ALrclgxB4J",
        "outputId": "7f1055a5-0263-4ef5-8882-2a058c842bd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Companies listed on the New York Stock Exchange (Q)\n",
            "\n",
            "==Q==\n",
            "\n",
            "{{Complete list|date=July 2023}}\n",
            "{| style=\"background:transparent;\"\n",
            "!Stock name\n",
            "!Symbol\n",
            "!Country of origin\n",
            "|----\n",
            "|Q2 Holdings, Inc.\n",
            "|{{nyse1|QTWO}}\n",
            "|US\n",
            "|----\n",
            "|[[Qiagen|Qiagen N.V.]]\n",
            "|{{nyse1|QGEN}}\n",
            "|[[Netherlands]]\n",
            "|----\n",
            "|[[Quad (company)|Quad/Graphics, Inc.]]\n",
            "|{{nyse1|QUAD}}\n",
            "|US\n",
            "|----\n",
            "|[[Quaker Chemical Corporation]]\n",
            "|{{nyse1|KWR}}\n",
            "|US\n",
            "|----\n",
            "|Quanex Building Products Corporation\n",
            "|{{nyse1|NX}}\n",
            "|US\n",
            "|----\n",
            "|[[Quanta Services|Quanta Services Inc.]]\n",
            "|{{nyse1|PWR}}\n",
            "|US\n",
            "|----\n",
            "|Quantum FinTech Acquisition Corporation\n",
            "|{{nyse1|QFTA}}\n",
            "|US\n",
            "|----\n",
            "|Quantum FinTech Acquisition Corporation\n",
            "|{{nyse1|QFTA.UN}}\n",
            "|US\n",
            "|----\n",
            "|[[QuantumScape|QuantumScape Corporation]]\n",
            "|{{nyse1|QS}}\n",
            "|US\n",
            "|----\n",
            "|Qudian Inc.\n",
            "|{{nyse1|QD}}\n",
            "|[[China]]\n",
            "|----\n",
            "|[[Quest Diagnostics|Quest Diagnostics Incorporated]]\n",
            "|{{nyse1|DGX}}\n",
            "|US\n",
            "|----\n",
            "|[[Quotient Technology|Quotient Technology Inc.]]\n",
            "|{{nyse1|QUOT}}\n",
            "|US\n",
            "|----\n",
            "|[[QVC|QVC, Inc.]]\n",
            "|{{nyse1|QVCC}}\n",
            "|US\n",
            "|----\n",
            "|QVC, Inc.\n",
            "|{{nyse1|QVCD}}\n",
            "|US\n",
            "|----\n",
            "|[[Qwest Corporation]]\n",
            "|{{nyse1|CTBB}}\n",
            "|US\n",
            "|----\n",
            "|Qwest Corporation\n",
            "|{{nyse1|CTDD}}\n",
            "|US\n",
            "|----\n",
            "|}\n",
            "\n",
            "[[Category:Lists of companies listed on the New York Stock Exchange|Q]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# print example data\n",
        "print(wikipedia_strings[1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jitleEDDxB4K"
      },
      "source": [
        "## 3. Embed document chunks\n",
        "\n",
        "Now that we've split our library into shorter self-contained strings, we can compute embeddings for each.\n",
        "\n",
        "(For large embedding jobs, use a script like [api_request_parallel_processor.py](api_request_parallel_processor.py) to parallelize requests while throttling to stay under rate limits.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3tY5s5bxB4K",
        "outputId": "2de8dbb8-6b02-438e-f5b8-d336faf8a37a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0 to 999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "APIRemovedInV1",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-beb219b2862a>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwikipedia_strings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_end\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Batch {batch_start} to {batch_end-1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEMBEDDING_MODEL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"index\"\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# double check embeddings are in the same order as input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_utils/_proxy.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_proxied__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_utils/_proxy.py\u001b[0m in \u001b[0;36m__get_proxied__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_proxied__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_cache\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mproxied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__proxied\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/lib/_old_api.py\u001b[0m in \u001b[0;36m__load__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__load__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAPIRemovedInV1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_symbol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.Embedding, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
          ]
        }
      ],
      "source": [
        "# calculate embeddings\n",
        "EMBEDDING_MODEL = \"text-embedding-ada-002\"  # OpenAI's best embeddings as of Apr 2023\n",
        "BATCH_SIZE = 1000  # you can submit up to 2048 embedding inputs per request\n",
        "\n",
        "embeddings = []\n",
        "for batch_start in range(0, len(wikipedia_strings), BATCH_SIZE):\n",
        "    batch_end = batch_start + BATCH_SIZE\n",
        "    batch = wikipedia_strings[batch_start:batch_end]\n",
        "    print(f\"Batch {batch_start} to {batch_end-1}\")\n",
        "    response = openai.Embedding.create(model=EMBEDDING_MODEL, input=batch)\n",
        "    for i, be in enumerate(response[\"data\"]):\n",
        "        assert i == be[\"index\"]  # double check embeddings are in same order as input\n",
        "    batch_embeddings = [e[\"embedding\"] for e in response[\"data\"]]\n",
        "    embeddings.extend(batch_embeddings)\n",
        "\n",
        "df = pd.DataFrame({\"text\": wikipedia_strings, \"embedding\": embeddings})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVF9x_XfxB4L"
      },
      "source": [
        "## 4. Store document chunks and embeddings\n",
        "\n",
        "Because this example only uses a few thousand strings, we'll store them in a CSV file.\n",
        "\n",
        "(For larger datasets, use a vector database, which will be more performant.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xnYwTTRxB4L"
      },
      "outputs": [],
      "source": [
        "# save document chunks and embeddings\n",
        "\n",
        "SAVE_PATH = \"data/winter_olympics_2022.csv\"\n",
        "\n",
        "df.to_csv(SAVE_PATH, index=False)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "openai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
